# 电商库存补货 RL 配置文件（v3 - 参考rl_0113基线优化）
# 目标: RTS≤2.4%, ACC≥80%
# 核心改进:
#   1. 添加 lt_begin_stock/lt_demand 关键状态特征（基线最重要的设计）
#   2. 奖励重写为基线验证过的 bind + estimate_rts + safe_stock 设计
#   3. PPO超参数对齐基线（k_epochs=4, grad_norm=1.0）
#   4. 使用更细的 stock_days 步长（0.5）增加动作分辨率

task:
  name: "replenishment_v2"
  seed: 42
  device: "auto"

data:
  train_path: "../data/2000_sku.parquet"
  eval_path: "../data/2000_sku.parquet"

env:
  rts_days: 14
  max_leadtime: 5
  state_features:
    dynamic:
      - "current_stock"
      - "transit_day_1"
      - "transit_day_2"
      - "transit_day_3"
      - "transit_day_4"
      - "transit_day_5"
      - "total_transit"
      - "lt_begin_stock"        # 新增：前向模拟到货时库存（最关键的基线特征）
      - "lt_demand"             # 新增：到货日预测需求
      - "stock_health"
      - "days_of_stock"
      - "near_expire_ratio"
    static:
      - "leadtime"
      - "pred_y"
      - "demand_freq"
      - "order_ratio_l7d"
      - "order_ratio_l14d"
      - "avg_daily_item_qty_l7d"
      - "avg_daily_item_qty_l14d"
      - "std_daily_item_qty_l7d"
      - "std_daily_item_qty_l14d"
      - "trend_item_qty_l3d_minus_l7d"
      - "trend_item_qty_l3d_minus_l14d"

# ============ 动作配置 ============
action:
  type: "discrete"
  policy_regularization: "none"
  # stock_days 模式: 输出目标库存天数
  action_mode: "stock_days"
  stock_days_range: [0, 7]       # 扩大到7天
  stock_days_step: 0.5           # 更细粒度 → 15个动作
  max_replenish_days: 7
  # 备用乘数配置
  multiplier_range: [0.3, 0.8]
  multiplier_step: 0.1
  target_stock_multiplier_range: [0, 4]
  target_stock_step: 1

# ============ 奖励配置（参考基线 with_safe_stock）============
reward:
  type: "balanced"
  weights:
    bind: 0.3
    rts: 0.45
    overnight: 0.005
    safe_stock: 0.5
    stockout: 0.6
  targets:
    safe_stock_standard: 1.2
    head_sku_threshold: 0.3
  use_terminal_reward: false     # 禁用终止态奖励，依赖逐步信号
  normalize: false
  clip_range: [-10, 10]

# ============ 网络配置（简化，匹配基线复杂度）============
network:
  hidden_dims: [128, 128]        # 简化网络（基线为128）
  activation: "relu"
  use_layer_norm: true
  use_residual: false
  dropout: 0.0
  init_method: "orthogonal"
  init_gain: 1.0

# ============ PPO 配置（对齐基线超参数）============
ppo:
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coef: 0.015            # 适度探索（15个动作空间）
  value_loss_coef: 0.5
  max_grad_norm: 1.0             # 基线=1.0（之前0.5太保守）
  lr_actor: 0.0003
  lr_critic: 0.0003
  weight_decay: 0.0
  k_epochs: 4                   # 基线=3，折中取4
  batch_size: 2048               # 减小以加速更新
  mini_batch_size: 512
  normalize_advantages: true
  advantage_clip: 5.0
  log_ratio_clip: 20.0
  diagnose_interval: 50
  use_lr_scheduler: true
  lr_decay_step: 80
  lr_decay_gamma: 0.95

# ============ 训练配置 ============
training:
  max_episodes: 300
  eval_interval: 10
  save_interval: 50
  early_stop_patience: 50
  target_rts: 2.4
  target_acc: 80.0
  accumulate_episodes: 1         # 每个episode更新一次（基线风格）
  use_terminal_reward: false
  use_state_norm: true
  use_reward_norm: true          # 启用奖励归一化（处理不同SKU的规模差异）
  norm_clip: 10.0
  curriculum:
    enabled: true
    stages:
      - name: "warmup_bind"
        episodes: 50
        rts_weight: 0.35
        bind_weight: 0.3
        overnight_weight: 0.005
        safe_stock_weight: 0.4
      - name: "balance"
        episodes: 100
        rts_weight: 0.45
        bind_weight: 0.3
        overnight_weight: 0.005
        safe_stock_weight: 0.5
      - name: "acc_focus"
        episodes: 150
        rts_weight: 0.45
        bind_weight: 0.3
        overnight_weight: 0.005
        safe_stock_weight: 0.5

logging:
  log_dir: "output"
  tensorboard: true
  print_interval: 5
  save_best_only: true
