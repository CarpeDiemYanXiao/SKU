# 优化日志

## 项目目标
- **Baseline**: ACC ~75%, RTS ~2.4%
- **Target**: ACC ≥80%, RTS ≤2.4% (ACC提升≥5%, RTS不升高)

---

## 实验记录

### Exp-001: 基础架构搭建
**日期**: 2026-02-04

**改动**:
1. 重构代码结构，模块化设计
2. 实现更清晰的 State/Action/Reward 分离
3. 添加课程学习支持

**配置**:
```yaml
action: discrete, multiplier [0.8, 3.0], step 0.1
reward_weights: bind=0.25, rts=1.0, overnight=0.02, stockout=0.15
network: [256, 256, 128] + LayerNorm + Residual
ppo: lr=3e-4, k_epochs=5, batch=2048
```

**结果**: 待测试

---

### 优化方向规划

#### 1. Reward Engineering (高优先级)
- [ ] 增加缺货惩罚权重
- [ ] 尝试分层 Reward (先保RTS，再优化ACC)
- [ ] 自适应 Reward (按SKU频率调整)

#### 2. State 设计优化 (中优先级)
- [ ] 添加库存健康度特征
- [ ] 添加可售天数特征
- [ ] 更好的特征归一化

#### 3. 网络结构优化 (中优先级)
- [ ] 尝试更深/更宽的网络
- [ ] 添加注意力机制
- [ ] Dueling 架构

#### 4. 训练策略优化 (高优先级)
- [ ] 课程学习: 先简单SKU，后复杂SKU
- [ ] 分阶段训练: 先低RTS，后高ACC
- [ ] 探索策略调整

#### 5. Action 空间优化 (低优先级)
- [ ] 尝试更细粒度的 multiplier
- [ ] 尝试直接输出补货量

---

## 调参记录表

| 实验ID | 日期 | 主要改动 | ACC | RTS | 备注 |
|--------|------|----------|-----|-----|------|
| baseline | - | 原始模型 | 75.0% | 2.4% | 基准线 |
| exp-001 | 2026-02-04 | 重构代码 | - | - | 待测试 |

---

## 踩坑记录

### 1. 状态归一化
**问题**: 不同特征量级差异大，导致训练不稳定
**解决**: 使用 LayerNorm + 特征级别归一化

### 2. Reward 设计
**问题**: RTS惩罚过大导致策略过于保守，ACC下降
**解决**: 使用分层Reward，在RTS约束内最大化ACC

### 3. 多SKU训练
**问题**: 不同SKU特性差异大，单一策略难以适应
**解决**: 考虑添加SKU嵌入或条件策略

---

## 下一步计划

1. 运行基础实验，获取 baseline
2. 调整 Reward 权重
3. 实现课程学习
4. 特征工程优化
