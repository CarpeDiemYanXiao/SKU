
import logging
import os
import torch.distributed as dist
import torch.multiprocessing as mp
import argparse
from pathlib import Path
from trainer import TrainerConfig
from task import task_dict
from utils.io import read_json
from utils.log import logging_once
from a_refactor_train import Trainer


def setup_cpu(rank, world_size, master_addr="localhost", master_port="12355", socket_ifname="lo0"):
    """
    åœ¨CPUä¸Šåˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    """
    os.environ["MASTER_ADDR"] = master_addr
    os.environ["MASTER_PORT"] = master_port
    os.environ["GLOO_SOCKET_IFNAME"] = socket_ifname  # åŠ è¿™è¡Œï¼Œé˜²æ­¢åœ¨å¤šèŠ‚ç‚¹è®­ç»ƒæ—¶ï¼Œä¸åŒèŠ‚ç‚¹ä¹‹é—´é€šä¿¡å¤±è´¥
    dist.init_process_group(
        backend="gloo",
        rank=rank,
        world_size=world_size,
    )


def cleanup():
    """
    æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    """
    dist.destroy_process_group()


def run_one_process(rank, world_size, config, master_addr="127.0.0.1", master_port="12355"):
    """
    åœ¨CPUä¸Šè¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    """
    try:
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        setup_cpu(rank, world_size, master_addr, master_port)
        config.distributed = True if world_size > 1 else False
        # åœ¨è¿™é‡Œå·²ç»è®²rankå†™è¿›configäº†ï¼Œæ¯ä¸ªè¿›ç¨‹çš„rankéƒ½ä¸ä¸€æ ·çš„
        config.rank = rank
        # å°†deviceè®¾ç½®ä¸ºcpu
        config.device = "cpu"
        config.world_size = world_size
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
        trainer = Trainer(config)
        trainer.train()   
    except Exception as e:
        logging_once(f"Error in process {rank}: {e}", logging.CRITICAL)
        raise e
    finally:
        cleanup()


def str2bool(str):
    return True if str.lower() == "true" else False


def arg_parser():
    # åˆ†å¸ƒå¼ç›¸å…³è®¾ç½®
    parser = argparse.ArgumentParser(description="CPU Distributed ReplenishAgent Training")
    parser.add_argument("--num_processes", type=int, default=4, help="Number of processes per node")
    parser.add_argument("--master_addr", type=str, default="127.0.0.1", help="Master node address")
    parser.add_argument("--master_port", type=str, default="12355", help="Master node port")
    parser.add_argument("--node_rank", type=int, default=0, help="Rank of this node")
    parser.add_argument("--num_nodes", type=int, default=1, help="Total number of nodes")

    # è®­ç»ƒç‰ˆæœ¬ç›¸å…³è®¾ç½®
    parser.add_argument("--task_name", type=str, required=True, help="ä»»åŠ¡å")
    parser.add_argument("--data_ver",type=str,required=True,help="æ•°æ®ç‰ˆæœ¬,ä»»åŠ¡å+æ•°æ®ç‰ˆæœ¬=æ•°æ®æ–‡ä»¶å¤¹,æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®éƒ½æ”¾åœ¨æ•°æ®æ–‡ä»¶å¤¹ä¸‹é¢")
    parser.add_argument("--para_ver", type=str, required=True, help="å®éªŒç‰ˆæœ¬,æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®åœ¨æ•°æ®æ–‡ä»¶å¤¹ä¸‹é¢,ç”±å®éªŒç‰ˆæœ¬ä½œä¸ºå¼€å¤´")
    parser.add_argument("--json_path", type=str, default="", help="æŸäº›taskå¯èƒ½éœ€è¦ä»jsonä¸­è§£æéœ€è¦çš„å†…å®¹")
    # parser.add_argument(
    # "--json_path_ls", 
    # type=lambda x: [s.strip() for s in x.split(",") if s.strip()],
    # default=[],
    # help="é€—å·åˆ†éš”çš„ JSON è·¯å¾„åˆ—è¡¨ï¼Œå¦‚: path1.json,path2.json"
    # )
    # configåŸæœ‰å‚æ•°ï¼Œä¸ä¼ å…¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
    parser.add_argument("--data_path",type=str,default=argparse.SUPPRESS,help="åŸå§‹è®­ç»ƒæ•°æ®,å¦‚æœæ˜¯æ–‡ä»¶åå°±åœ¨æ•°æ®æ–‡ä»¶å¤¹æŸ¥æ‰¾,æ˜¯ç»å¯¹è·¯å¾„å°±")
    parser.add_argument("--valid_data_path",type=str,default=argparse.SUPPRESS,help="æµ‹è¯•é›†æ•°æ®")
    parser.add_argument("--model_name", type=str, default=argparse.SUPPRESS, help="æœ¬æ¬¡è®­ç»ƒä½¿ç”¨çš„æ¨¡å‹åç§°base_ppo,ppo_continue_action,dqn")
    parser.add_argument("--optim_name", type=str, default=argparse.SUPPRESS, help="ä¼˜åŒ–å™¨è®¾ç½®")
    parser.add_argument("--loss_name", type=str, default=argparse.SUPPRESS, help="policyæŸå¤±å‡½æ•°çš„åç§°")
    parser.add_argument("--value_loss_name", type=str, default=argparse.SUPPRESS, help="valueæ¨¡å‹çš„æŸå¤±å‡½æ•°åç§°")
    # è®­ç»ƒå‚æ•°,è‹¥ä¸ä¼ å…¥,åˆ™é‡‡ç”¨trainer/trainer_confä¸­çš„é»˜è®¤å€¼
    parser.add_argument("--k_epochs", type=int, default=argparse.SUPPRESS, help="æ¯ä¸ªepisodeä¸­é‡‡ç”¨çš„epochçš„è½®æ¬¡")
    parser.add_argument("--max_episodes", type=int, default=argparse.SUPPRESS, help="æœ€å¤§episodesè½®æ¬¡")
    parser.add_argument("--batch_size", type=int, default=argparse.SUPPRESS, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=argparse.SUPPRESS, help="å­¦ä¹ ç‡")
    parser.add_argument("--norm_clip", type=float, default=argparse.SUPPRESS, help="å½’ä¸€åŒ–æ—¶çš„æ¢¯åº¦è£å‰ª")
    parser.add_argument("--clip_grad", type=float, default=argparse.SUPPRESS, help="åå‘ä¼ æ’­çš„æ¢¯åº¦è£å‰ª")
    parser.add_argument("--clip_grad_decay", type=float, default=argparse.SUPPRESS, help="æ¢¯åº¦è£å‰ªçš„è¡°å‡")
    parser.add_argument("--print_every", type=int, default=argparse.SUPPRESS, help="lossæ‰“å°çš„é—´éš”")
    parser.add_argument("--sample", type=float, default=argparse.SUPPRESS, help="æ•°æ®çš„é‡‡æ ·æ¯”ä¾‹")
    parser.add_argument("--l2", type=float, default=argparse.SUPPRESS, help="l2æ­£åˆ™çš„æ•°")
    parser.add_argument("--save_every_eposide", type=int, default=argparse.SUPPRESS, help="ä¿å­˜æ¨¡å‹çš„é—´éš”")

    parser.add_argument("--use_state_norm", type=str2bool, default=argparse.SUPPRESS, help="æ˜¯å¦ä½¿ç”¨stateå½’ä¸€åŒ–")
    parser.add_argument("--use_discount_reward_norm", type=str2bool, default=argparse.SUPPRESS, help="æ˜¯å¦ä½¿ç”¨rewardå½’ä¸€åŒ–")
    parser.add_argument("--center", type=str2bool, default=argparse.SUPPRESS, help="rewardå½’ä¸€åŒ–ç›¸å…³å‚æ•°")
    parser.add_argument("--scale", type=str2bool, default=argparse.SUPPRESS, help="scaleå½’ä¸€åŒ–ç›¸å…³å‚æ•°")
    parser.add_argument("--use_checkpoint", type=str2bool, default=False, help="æ˜¯å¦é‡‡ç”¨checkpointè®­ç»ƒ")
    parser.add_argument("--checkpoint_path", type=str, default=argparse.SUPPRESS, help="checkpointè·¯å¾„")
    parser.add_argument("--continue_mode", type=str, default='pretrain', help="ç»§ç»­è®­ç»ƒæ¨¡å¼ï¼Œresumeæˆ–pretrain")
    # parser.add_argument("--checkpoint_name_value", type=str, default='', help="valueçš„checkpointè·¯å¾„")
    parser.add_argument("--action_ls",type=lambda x: list(map(float, x.split(","))),default=argparse.SUPPRESS,help="æ ‡ç­¾çš„æ˜ å°„å…³ç³»,ä¸€èˆ¬éƒ½æ¯”è¾ƒé•¿,ä¸å»ºè®®åœ¨argsä¸­ä¼ å…¥")

    args = parser.parse_args()
    args.base_dir = str(Path(__file__).parents[0])  # project dir
    print(str(Path(__file__).parents[0]))
    print(args.base_dir)
    return args

def run_one_process_curriculum(rank, world_size, config_ls, master_addr, master_port):
    """è¯¾ç¨‹å­¦ä¹ ç‰ˆæœ¬çš„è®­ç»ƒå…¥å£"""
    try:
        setup_cpu(rank, world_size, master_addr, master_port)
        
        # ä¸ºæ‰€æœ‰ config è®¾ç½®åˆ†å¸ƒå¼å‚æ•°
        for config in config_ls:
            config.distributed = world_size > 1
            config.rank = rank
            config.device = "cpu"
            config.world_size = world_size
        
        trainer = Trainer(config_ls)
        trainer.train_curriculum()
    except Exception as e:
        logging_once(f"Error in process {rank}: {e}", logging.CRITICAL)
        raise e
    finally:
        cleanup()

def run_one_process_curriculum_new(rank, world_size, config, master_addr, master_port):
    """è¯¾ç¨‹å­¦ä¹ ç‰ˆæœ¬çš„è®­ç»ƒå…¥å£"""
    try:
        setup_cpu(rank, world_size, master_addr, master_port)
        
        # ä¸ºæ‰€æœ‰ config è®¾ç½®åˆ†å¸ƒå¼å‚æ•°
        
        config.distributed = world_size > 1
        config.rank = rank
        config.device = "cpu"
        config.world_size = world_size
        
        trainer = Trainer(config)
        trainer.train_curriculum()
    except Exception as e:
        logging_once(f"Error in process {rank}: {e}", logging.CRITICAL)
        raise e
    finally:
        cleanup()

def main():
    # # åˆå§‹åŒ–argså‚æ•°
    # args = arg_parser()
    # # åˆå§‹åŒ–taskå‚æ•°
    # config = task_dict[args.task_name](args.json_path)
    # if args.json_path:
    #     config.update(read_json(args.json_path), priority="high")
    # config.update(args, priority="high")
    # config.update(TrainerConfig(), priority="low")
    # config.update(modelconfig_dict[config.model_name](), priority="low")

    # config.initialize()
    # # processes_per_node = args.num_processes  # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°
    # config.world_size = config.num_nodes * config.num_processes  # æ€»è¿›ç¨‹æ•° = èŠ‚ç‚¹æ•° Ã— æ¯èŠ‚ç‚¹è¿›ç¨‹æ•°
    # config.node_rank_start = config.node_rank * config.num_processes  # å½“å‰èŠ‚ç‚¹çš„èµ·å§‹è¿›ç¨‹rank

    # print(f"Node {config.node_rank}/{config.num_nodes}: Starting training with {config.num_processes} processes")
    # print(f"Global ranks from {config.node_rank_start} to {config.node_rank_start + config.num_processes - 1}")
    # print(f"Process num {config.world_size} using CPU")
    # # æ‰“å°config
    # logging_once(config, logging.CRITICAL)
    # print(f"ğŸ˜ºğŸ˜ºstateç»´åº¦:{config.state_dim}")

    # if config.world_size == 1:
    #     run_one_process(0, 1, config, config.master_addr, config.master_port)  # å•è¿›ç¨‹è®­ç»ƒ
    # else:
    #     processes = []  # å¤šè¿›ç¨‹è®­ç»ƒ - æ¯ä¸ªèŠ‚ç‚¹åªå¯åŠ¨è‡ªå·±è´Ÿè´£çš„é‚£éƒ¨åˆ†è¿›ç¨‹
    #     for local_rank in range(config.num_processes):
    #         global_rank = config.node_rank_start + local_rank  # è®¡ç®—å…¨å±€rank
    #         p = mp.Process(
    #             target=run_one_process,
    #             args=(global_rank, config.world_size, config, config.master_addr, config.master_port),
    #         )
    #         p.start()
    #         processes.append(p)

    #     for p in processes:  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    #         p.join()
    
    # # åˆå§‹åŒ–argså‚æ•°
    # args = arg_parser()
    # # è¯¾ç¨‹å­¦ä¹ ï¼šè§£æå¤šä¸ª JSON æ–‡ä»¶ç”Ÿæˆ config åˆ—è¡¨
    # config_ls = []
    # for idx, json_path in enumerate(args.json_path_ls):
    #     config = task_dict[args.task_name](json_path)
    #     if json_path:
    #         config.update(read_json(json_path), priority="high")
    #     config.update(args, priority="high")
    #     config.update(TrainerConfig(), priority="low")
    #     config.update(modelconfig_dict[config.model_name](), priority="low")
        
    #     # ä¸ºæ¯ä¸ªé˜¶æ®µè®¾ç½®æ ‡è¯†
    #     config.stage_idx = idx
    #     config.total_stages = len(args.json_path_ls)
        
    #     config.initialize()
    #     config_ls.append(config)
    
    # # ä½¿ç”¨ç¬¬ä¸€ä¸ª config åˆå§‹åŒ–åˆ†å¸ƒå¼ç›¸å…³å‚æ•°
    # base_config = config_ls[0]
    # base_config.world_size = base_config.num_nodes * base_config.num_processes
    # base_config.node_rank_start = base_config.node_rank * base_config.num_processes

    # if base_config.world_size == 1:
    #     run_one_process_curriculum(0, 1, config_ls, base_config.master_addr, base_config.master_port)
    # else:
    #     processes = []
    #     for local_rank in range(base_config.num_processes):
    #         global_rank = base_config.node_rank_start + local_rank
    #         p = mp.Process(
    #             target=run_one_process_curriculum,
    #             args=(global_rank, base_config.world_size, config_ls, 
    #                   base_config.master_addr, base_config.master_port),
    #         )
    #         p.start()
    #         processes.append(p)
    #     for p in processes:
    #         p.join()
    

    # åˆå§‹åŒ–argså‚æ•°
    args = arg_parser()
    # åˆå§‹åŒ–taskå‚æ•°
    config = task_dict[args.task_name](args.json_path)
    if args.json_path:
        config.update(read_json(args.json_path), priority="high")
    config.update(args, priority="high")
    config.update(TrainerConfig(), priority="low")

    if args.data_path:
        for stage in config.curriculum_stages:
            stage['data_path'] = args.data_path
    

    config.initialize()
    # processes_per_node = args.num_processes  # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°
    config.world_size = config.num_nodes * config.num_processes  # æ€»è¿›ç¨‹æ•° = èŠ‚ç‚¹æ•° Ã— æ¯èŠ‚ç‚¹è¿›ç¨‹æ•°
    config.node_rank_start = config.node_rank * config.num_processes  # å½“å‰èŠ‚ç‚¹çš„èµ·å§‹è¿›ç¨‹rank

    print(f"Node {config.node_rank}/{config.num_nodes}: Starting training with {config.num_processes} processes")
    print(f"Global ranks from {config.node_rank_start} to {config.node_rank_start + config.num_processes - 1}")
    print(f"Process num {config.world_size} using CPU")
    # æ‰“å°config
    logging_once(config, logging.CRITICAL)
    print(f"ğŸ˜ºğŸ˜ºstateç»´åº¦:{config.state_dim}")

    if config.world_size == 1:
        run_one_process_curriculum_new(0, 1, config, config.master_addr, config.master_port)  # å•è¿›ç¨‹è®­ç»ƒ
    else:
        processes = []  # å¤šè¿›ç¨‹è®­ç»ƒ - æ¯ä¸ªèŠ‚ç‚¹åªå¯åŠ¨è‡ªå·±è´Ÿè´£çš„é‚£éƒ¨åˆ†è¿›ç¨‹
        for local_rank in range(config.num_processes):
            global_rank = config.node_rank_start + local_rank  # è®¡ç®—å…¨å±€rank
            p = mp.Process(
                target=run_one_process_curriculum_new,
                args=(global_rank, config.world_size, config, config.master_addr, config.master_port),
            )
            p.start()
            processes.append(p)

        for p in processes:  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
            p.join()


if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼
    mp.set_start_method("spawn", force=True)
    main()