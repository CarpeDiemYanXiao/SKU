{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "from utils.datasets import Datasets\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.distributions import Categorical\n",
    "from network.PolicyNetwork import PolicyNetwork\n",
    "from network.ValueNetwork import ValueNetwork\n",
    "from utils.datasets import Datasets\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class Datasets:\n",
    "    def __init__(self, file_path: str, rank: int = 0, world_size: int = 1) -> None:\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # TODO 通过sku信息加载部分结果, 不要全量加载到内存\n",
    "        print(f\"Process {rank}/{world_size}: #############load dataset#############\")\n",
    "        if file_path.endswith(\".csv\"):\n",
    "            input_data = pd.read_csv(file_path)\n",
    "        else:\n",
    "            input_data = pd.read_parquet(file_path)\n",
    "        if type(input_data[\"predicted_demand\"].tolist()[0]) == str:\n",
    "            input_data[\"predicted_demand\"] = input_data[\"predicted_demand\"].apply(eval)\n",
    "        # input_data = input_data[input_data[\"idx\"] == \"257082622036-4894\"]\n",
    "\n",
    "        input_data = input_data.sort_values(by=[\"idx\", \"date\"])\n",
    "        print(f\"Process {rank}/{world_size}: #############data loaded#############\")\n",
    "\n",
    "        unique_idx = input_data[\"idx\"].unique()\n",
    "        if world_size > 1:\n",
    "            # 计算该进程应处理的idx范围\n",
    "            chunk_size = len(unique_idx) // world_size\n",
    "            start_idx = rank * chunk_size\n",
    "            end_idx = start_idx + chunk_size if rank < world_size - 1 else len(unique_idx)\n",
    "            process_idx_list = unique_idx[start_idx:end_idx]\n",
    "            input_data = input_data[input_data[\"idx\"].isin(process_idx_list)]\n",
    "            print(f\"Process {rank}/{world_size}: handle {len(process_idx_list)} SKU (Total: {len(unique_idx)})\")\n",
    "        print(f\"Process {rank}/{world_size}: #############gen data#############\")\n",
    "\n",
    "        idx_data = input_data.groupby(\"idx\")\n",
    "        self.sales_map = idx_data[\"actual\"].apply(list).to_dict()\n",
    "        self.total_sales = input_data[\"actual\"].sum()\n",
    "\n",
    "        self.predicts_map = idx_data[\"predicted_demand\"].apply(lambda s: np.array(s.tolist())).to_dict()\n",
    "\n",
    "        self.end_date_map = idx_data[\"date\"].count().to_dict()\n",
    "        self.leadtime_map = idx_data[\"leadtime\"].apply(list).to_dict()\n",
    "        self.predict_leadtime_day = idx_data[\"pred_y\"].apply(list).to_dict()\n",
    "        self.initial_stock_map = idx_data[\"initial_stock\"].max().to_dict()\n",
    "\n",
    "        date_idx_data = input_data.groupby([\"date\", \"idx\"])\n",
    "        self.sku_id_ls = date_idx_data[\"pred_y\"].sum().unstack().columns.tolist()\n",
    "        self.predicted_demand = date_idx_data[\"pred_y\"].sum().unstack().fillna(0).values\n",
    "        print(f\"Process {rank}/{world_size}: #############gen data done#############\")\n",
    "\n",
    "    def get_initial_stock_map(self):\n",
    "        return self.initial_stock_map\n",
    "\n",
    "    def get_end_date_map(self):\n",
    "        return self.end_date_map\n",
    "\n",
    "    def sku_ids(self) -> list:\n",
    "        return self.sku_id_ls\n",
    "\n",
    "    def sku_lead_time(self, sku_id: int, day_idx: int) -> int:\n",
    "        return self.leadtime_map.get(sku_id, [1])[day_idx]\n",
    "\n",
    "    def range_lead_time(self, day_st, day_ed, sku_id) -> list:\n",
    "        return self.leadtime_map[sku_id][day_st:day_ed]\n",
    "\n",
    "    def range_prdicts(self, day_st, day_ed, sku_id) -> list:\n",
    "        return self.predicts_map[sku_id][day_st:day_ed]\n",
    "\n",
    "    def get_predicts(self, day_idx: int, sku_id: int) -> list:\n",
    "        return self.predicts_map[sku_id][day_idx].tolist()\n",
    "\n",
    "    def get_leadtime_predict(self, day_idx: int, sku_id: int) -> list:\n",
    "        return self.predict_leadtime_day[sku_id][day_idx]\n",
    "\n",
    "    def range_sales(self, day_st, day_ed, sku_id) -> list:\n",
    "        return self.sales_map[sku_id][day_st:day_ed]\n",
    "\n",
    "    def get_sales(self, day_idx: int, sku_id: int):\n",
    "        return self.sales_map[sku_id][day_idx]\n",
    "\n",
    "\n",
    "class ReplenishAgent_make_date:\n",
    "    def __init__(self, replenish_model, config):\n",
    "        self.replenish_model = replenish_model\n",
    "        self.task_name = config[\"task_name\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.state_label = config[\"state_label\"]\n",
    "        # 分布式训练参数\n",
    "        self.distributed = False\n",
    "        self.rank = 0\n",
    "        self.world_size = 1\n",
    "        self.is_master = self.rank == 0\n",
    "\n",
    "        # 检查device是否为数字（GPU索引）并处理\n",
    "        if isinstance(self.device, int) and not torch.cuda.is_available():\n",
    "            print(f\"Warning: CUDA not available, switching to CPU\")\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        ###输入的数据\n",
    "        ###商品清单\n",
    "        self.datasets = Datasets(config[\"data_path\"], self.rank, self.world_size)\n",
    "        self.sku_id_ls = self.datasets.sku_ids()\n",
    "        self.predict_leadtime_day = self.datasets.predict_leadtime_day\n",
    "        self.leadtime_map = self.datasets.leadtime_map\n",
    "        self.initial_stock_map = self.datasets.get_initial_stock_map()\n",
    "        self.end_date_map = self.datasets.get_end_date_map()\n",
    "        self.total_sales = self.datasets.total_sales\n",
    "        self.sales_map = self.datasets.sales_map\n",
    "\n",
    "        ###network 配置\n",
    "        self.state_dim = config[\"state_dim\"]\n",
    "        self.multiplier_ls: list[float] = config[\"multiplier_ls\"]\n",
    "        self.action_dim = len(config[\"multiplier_ls\"])\n",
    "        self.policy = PolicyNetwork(self.state_dim, self.action_dim).to(self.device)\n",
    "        self.value = ValueNetwork(self.state_dim).to(self.device)\n",
    "\n",
    "        # # 如果使用分布式训练，将模型包装为DDP模型\n",
    "        # if self.distributed:\n",
    "        #     torch.manual_seed(42 + self.rank)\n",
    "        #     # 检查是否可以使用CUDA\n",
    "        #     if torch.cuda.is_available() and self.device != \"cpu\":\n",
    "        #         self.policy = DDP(self.policy, device_ids=[self.device], output_device=self.device)\n",
    "        #         self.value = DDP(self.value, device_ids=[self.device], output_device=self.device)\n",
    "        #     else:\n",
    "        #         # CPU上使用DDP\n",
    "        #         self.policy = DDP(self.policy)\n",
    "        #         self.value = DDP(self.value)\n",
    "\n",
    "        # # 仅在主进程中创建目录和处理文件\n",
    "        # if not self.distributed or self.is_master:\n",
    "        #     os.makedirs(os.path.join(\"output\", self.task_name), exist_ok=True)\n",
    "\n",
    "        # self.policy_model_filename = os.path.join(\"output\", self.task_name, f\"repl_policy_model.pth\")\n",
    "        # self.value_model_filename = os.path.join(\"output\", self.task_name, f\"repl_value_model.pth\")\n",
    "\n",
    "        # ###模型训练参数\n",
    "        # adjusted_lr = config[\"lr\"] * math.sqrt(self.world_size) if self.distributed else config[\"lr\"]\n",
    "\n",
    "        # self.optimizer = optim.Adam(self.policy.parameters(), lr=adjusted_lr)\n",
    "        # self.value_optimizer = optim.Adam(self.value.parameters(), lr=adjusted_lr)\n",
    "        # self.gamma = config[\"gamma\"]\n",
    "        # self.k_epochs = config[\"k_epochs\"]  ###k_epochs\n",
    "        # self.eps_clip = config[\"eps_clip\"]  ###eps_clip\n",
    "        # self.batch_size = config[\"batch_size\"]  ###batch_size\n",
    "        # self.max_episodes = config[\"max_episodes\"]\n",
    "\n",
    "        # 存储训练数据\n",
    "        ###v {sku:[] for sku in self.sku_id_ls}\n",
    "        self.states_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.actions_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        # self.logprobs_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        # self.state_values_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        # self.rewards_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        # self.dones_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        # self.episode_rewards_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        # self.episode_rewards = []\n",
    "\n",
    "        # if not self.distributed or self.is_master:\n",
    "        #     print(\"initialize done\")\n",
    "\n",
    "    def verify_ddp_gradients(self, model, rank):\n",
    "        \"\"\"打印并验证各层梯度\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                # 将梯度转换为CPU上的NumPy数组以便打印\n",
    "                grad_data = param.grad.detach().cpu().numpy()\n",
    "                grad_sum = np.sum(grad_data)\n",
    "                grad_mean = np.mean(grad_data)\n",
    "                print(f\"Rank {rank}, Layer {name}: grad_sum={grad_sum:.6f}, grad_mean={grad_mean:.6f}\")\n",
    "\n",
    "    def get_update_action(self, sku_id, state):\n",
    "        state, action, action_detach, logprob, state_value = self.select_action(state)\n",
    "        self.states_map[sku_id].append(state)\n",
    "        self.actions_map[sku_id].append(action_detach)\n",
    "        self.logprobs_map[sku_id].append(logprob)\n",
    "        self.state_values_map[sku_id].append(state_value)\n",
    "        return action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"选择动作\"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        probs = self.policy(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        logprob = m.log_prob(action).detach()\n",
    "\n",
    "        # return state, action.item(), action.detach(), logprob, state_value\n",
    "        value = self.value(state)\n",
    "        return state, action.item(), action.detach(), logprob, value\n",
    "\n",
    "    def select_action_deterministic(self, state):\n",
    "        \"\"\"确定性地选择动作，每次选择最优的动作\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            probs = self.policy(state)  # 解包返回值，只使用概率\n",
    "            action = torch.argmax(probs).item()\n",
    "        return action\n",
    "\n",
    "    def select_multiplier_deterministic(self, state):\n",
    "        \"\"\"调用模型选择最优的multiplier\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            probs = self.policy(state)  # 解包返回值，只使用概率\n",
    "            action = torch.argmax(probs).item()\n",
    "        return self.multiplier_ls[int(action)]\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        \"\"\"绘制奖励趋势图并保存\"\"\"\n",
    "        # 设置中文字体\n",
    "        # plt.rcParams[\"font.sans-serif\"] = [\"Arial Unicode MS\"]  # Mac系统\n",
    "        # plt.rcParams[\"axes.unicode_minus\"] = False  # 解决负号显示问题\n",
    "        print(f\"max episode_rewards = {max(self.episode_rewards)}\")\n",
    "        data = np.array(self.episode_rewards)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(data, linewidth=1)\n",
    "        # 使用英文标题避免字体问题\n",
    "        plt.title(\"Reward Trend for : \" + self.task_name)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        # 生成时间戳文件名并保存图片\n",
    "        from datetime import datetime\n",
    "\n",
    "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"output/{self.task_name}/reward_trend.jpg\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "    ###TODO\n",
    "    def gen_network_state_v1(self, state_dict, sku, day_idx):\n",
    "        \"\"\"\n",
    "        生成输入network的state：[end_of_stock,next_day_arrive_order,next_day_rts,forecast,leadtime]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ##天数\n",
    "        ###day_idx=state_dict[\"day_idx\"]\n",
    "        ##new_state=[day_idx,state_dict[\"end_of_stock\"],state_dict[\"arrival_abo\"],state[\"arrival_qty\"],self.predict_leadtime_day[sku][day_idx]]\n",
    "\n",
    "        transit_stock = state_dict[\"transit_stock\"] + [0] * (5 - len(state_dict[\"transit_stock\"]))\n",
    "        new_state = [\n",
    "            state_dict[\"end_of_stock\"],\n",
    "            ##state_dict[\"transit_stock\"],\n",
    "            0,##state_dict[\"next_day_rts\"],\n",
    "            self.predict_leadtime_day[sku][day_idx],\n",
    "            self.leadtime_map[sku][day_idx],\n",
    "            transit_stock[0],\n",
    "            transit_stock[1],\n",
    "            transit_stock[2],\n",
    "            transit_stock[3],\n",
    "            transit_stock[4],\n",
    "        ]\n",
    "        return new_state\n",
    "    def gen_network_state(self, state_dict, sku, day_idx):\n",
    "        \"\"\"\n",
    "        生成输入network的state：[end_of_stock,next_day_arrive_order,next_day_rts,forecast,leadtime]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ##天数\n",
    "        ###day_idx=state_dict[\"day_idx\"]\n",
    "        ##new_state=[day_idx,state_dict[\"end_of_stock\"],state_dict[\"arrival_abo\"],state[\"arrival_qty\"],self.predict_leadtime_day[sku][day_idx]]\n",
    "\n",
    "        transit_stock = state_dict[\"transit_stock\"] + [0] * (5 - len(state_dict[\"transit_stock\"]))\n",
    "        new_state = [\n",
    "            state_dict[\"end_of_stock\"],\n",
    "            ##state_dict[\"transit_stock\"],\n",
    "            state_dict[\"next_day_rts\"],\n",
    "            self.predict_leadtime_day[sku][day_idx],\n",
    "            self.leadtime_map[sku][day_idx],\n",
    "            transit_stock[0],\n",
    "            transit_stock[1],\n",
    "            transit_stock[2],\n",
    "            transit_stock[3],\n",
    "            transit_stock[4],\n",
    "        ]\n",
    "        return new_state\n",
    "\n",
    "    # def reset_network_state(self, sku):\n",
    "    #     \"\"\"\n",
    "    #     生成输入network的state\n",
    "    #     :return:期末库存，预测值\n",
    "    #     \"\"\"\n",
    "    #     ##return [0, 0, self.predict_leadtime_day[sku][0], self.leadtime_map[sku][0]]\n",
    "    #     return [\n",
    "    #         self.initial_stock_map[sku],\n",
    "    #         0,\n",
    "    #         self.predict_leadtime_day[sku][0],\n",
    "    #         self.leadtime_map[sku][0],\n",
    "    #         0,\n",
    "    #         0,\n",
    "    #         0,\n",
    "    #         0,\n",
    "    #         0,\n",
    "    #     ]\n",
    "    def reset_network_state(self, sku):\n",
    "        \"\"\"\n",
    "        生成输入network的state\n",
    "        :return:期末库存，预测值\n",
    "        \"\"\"\n",
    "        if self.state_label == \"with_rts_split\":\n",
    "            return [self.initial_stock_map[sku], 0, 0, 0, 0, 0, 0] + [self.leadtime_map[sku][0],self.predict_leadtime_day[sku][0]]\n",
    "        elif self.state_label == \"with_rts_combine\":\n",
    "            return [self.initial_stock_map[sku], 0, 0, 0, 0, 0] + [self.leadtime_map[sku][0],self.predict_leadtime_day[sku][0]]\n",
    "        else:\n",
    "            return [self.initial_stock_map[sku], 0, 0, 0, 0, 0] + [self.leadtime_map[sku][0],self.predict_leadtime_day[sku][0]]\n",
    "            \n",
    "    def reset_day_idx(self):\n",
    "        return 0\n",
    "\n",
    "    def reset_state_dict(self):\n",
    "        ###补货时间，期末库存，预测值\n",
    "        return {\"end_of_stock\": 0, \"arrival_abo\": 0, \"arrival_qty\": 0, \"day_idx\": 0}\n",
    "\n",
    "    def gen_update_data(self):\n",
    "\n",
    "        states_ls = []\n",
    "        actions_ls = []\n",
    "        logprobs_ls = []\n",
    "        advantages_ls = []\n",
    "        returns_ls = []\n",
    "        for sku in self.sku_id_ls:\n",
    "            ##print(len(self.states_map[sku]),self.leadtime_map[sku][0])\n",
    "            states = torch.stack(self.states_map[sku]).to(self.device)\n",
    "            actions = torch.tensor(self.actions_map[sku], dtype=torch.int64).to(self.device)\n",
    "            logprobs = torch.stack(self.logprobs_map[sku]).to(self.device)\n",
    "            state_values = torch.cat(self.state_values_map[sku]).squeeze().to(self.device)\n",
    "            rewards = torch.tensor(self.rewards_map[sku], dtype=torch.float32).to(self.device)\n",
    "            dones = torch.tensor(self.dones_map[sku], dtype=torch.float32).to(self.device)\n",
    "            returns = []\n",
    "            discounted_reward = 0\n",
    "            for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "                if done:\n",
    "                    discounted_reward = 0\n",
    "                discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "                returns.insert(0, discounted_reward)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "            advantages = returns - state_values.detach()\n",
    "\n",
    "            states_ls.append(states)\n",
    "            actions_ls.append(actions)\n",
    "            logprobs_ls.append(logprobs)\n",
    "            returns_ls.append(returns)\n",
    "            advantages_ls.append(advantages)\n",
    "\n",
    "        states_all = torch.cat(states_ls)\n",
    "        actions_all = torch.cat(actions_ls)\n",
    "        logprobs_all = torch.cat(logprobs_ls)\n",
    "        returns_all = torch.cat(returns_ls)\n",
    "        advantages_all = torch.cat(advantages_ls)\n",
    "        ### # 标准化优势值\n",
    "        advantages_all = (advantages_all - advantages_all.mean()) / (advantages_all.std() + 1e-8)\n",
    "\n",
    "        return states_all, actions_all, logprobs_all, returns_all, advantages_all\n",
    "\n",
    "    def select_action_batch(self, states_batch):\n",
    "        \"\"\"批量选择动作\"\"\"\n",
    "        probs, state_values = self.policy(states_batch)\n",
    "        m = Categorical(probs)\n",
    "        actions = m.sample()\n",
    "        logprobs = m.log_prob(actions)\n",
    "        values = self.value(states_batch)\n",
    "\n",
    "        return (states_batch, actions, actions.detach(), logprobs.detach(), values)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"更新策略网络\"\"\"\n",
    "        ###生成数据\n",
    "        states, actions, logprobs, returns, advantages = self.gen_update_data()\n",
    "\n",
    "        ### PPO更新\n",
    "        # 将所有经验数据合并为一个数据集\n",
    "        dataset = torch.utils.data.TensorDataset(states, actions, logprobs, advantages, returns)\n",
    "\n",
    "        # 在分布式训练中使用DistributedSampler\n",
    "        if self.distributed:\n",
    "            sampler = DistributedSampler(dataset, num_replicas=self.world_size, rank=self.rank)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, sampler=sampler)\n",
    "        else:\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # PPO更新\n",
    "        for epoch in range(self.k_epochs):\n",
    "            # 在每个epoch开始前设置sampler的epoch\n",
    "            if self.distributed:\n",
    "                dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "            for batch in dataloader:\n",
    "                (\n",
    "                    batch_states,\n",
    "                    batch_actions,\n",
    "                    batch_logprobs,\n",
    "                    batch_advantages,\n",
    "                    batch_returns,\n",
    "                ) = batch\n",
    "\n",
    "                batch_probs = self.policy(batch_states)  # 获取动作的概率分布\n",
    "\n",
    "                batch_values = self.value(batch_states)\n",
    "                m = Categorical(batch_probs)\n",
    "                batch_new_logprobs = m.log_prob(batch_actions)  ## # 计算采样动作的对数概率\n",
    "                entropy = m.entropy().mean()  ##计算分布的熵\n",
    "                # Finding the ratio\n",
    "                ratios = torch.exp(batch_new_logprobs - batch_logprobs)\n",
    "                # Finding Surrogate Loss\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                # final loss of clipped objective PPO\n",
    "                loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                loss_value = nn.MSELoss()(batch_values.squeeze(), batch_returns)\n",
    "\n",
    "                ###更新网络\n",
    "                self.optimizer.zero_grad()\n",
    "                self.value_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                loss_value.backward()\n",
    "                self.optimizer.step()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "            # 每个 epoch 保存一次模型，仅在主进程中进行\n",
    "            if not self.distributed or self.is_master:\n",
    "                self.save_model(self.policy_model_filename)\n",
    "                self.save_value_model(self.value_model_filename)\n",
    "\n",
    "        # 清空缓存\n",
    "        self.states_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.actions_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.logprobs_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.state_values_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.rewards_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.dones_map = {sku: [] for sku in self.sku_id_ls}\n",
    "        self.episode_rewards_map = {sku: [] for sku in self.sku_id_ls}\n",
    "\n",
    "\n",
    "    def cal_date(self, start_date, delta_days):\n",
    "        end_date = datetime.strptime(start_date, \"%Y-%m-%d\") + timedelta(days=delta_days)\n",
    "        return end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def gen_multipliers(self, actions_map, start_date, task_name):\n",
    "\n",
    "        key_ls = []\n",
    "        res_data_ls = []\n",
    "\n",
    "        for key, value in actions_map.items():\n",
    "            key_ls.append(key)\n",
    "            res_data_ls.append(value)\n",
    "\n",
    "        df = pd.DataFrame(data={\"idx\": key_ls, \"multilpier\": res_data_ls})\n",
    "\n",
    "        df[\"multiplier_index\"] = df[\"multilpier\"].apply(lambda x: list(range(len(x))))\n",
    "\n",
    "        exploded_df = df.explode([\"multilpier\", \"multiplier_index\"])\n",
    "        exploded_df[\"ds\"] = exploded_df.apply(lambda s: self.cal_date(start_date, s[\"multiplier_index\"]), axis=1)\n",
    "        exploded_df[\"dt_version\"] = task_name\n",
    "        exploded_df = exploded_df[[\"idx\", \"ds\", \"multilpier\", \"dt_version\"]]\n",
    "        exploded_df.columns = [\"idx\", \"ds\", \"multilpier\", \"dt_version\"]\n",
    "        return exploded_df\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"保存模型和网络配置\"\"\"\n",
    "        # 只在主进程保存模型\n",
    "        if self.distributed and not self.is_master:\n",
    "            return\n",
    "\n",
    "        model_state_dict = self.policy.module.state_dict() if self.distributed else self.policy.state_dict()\n",
    "        model_info = {\n",
    "            \"state_dict\": model_state_dict,\n",
    "            \"network_type\": \"\",\n",
    "            \"state_dim\": self.state_dim,\n",
    "            \"action_dim\": self.action_dim,\n",
    "        }\n",
    "        torch.save(model_info, path)\n",
    "\n",
    "    def save_value_model(self, path):\n",
    "        \"\"\"保存模型和网络配置\"\"\"\n",
    "        # 只在主进程保存模型\n",
    "        if self.distributed and not self.is_master:\n",
    "            return\n",
    "\n",
    "        model_state_dict = self.value.module.state_dict() if self.distributed else self.value.state_dict()\n",
    "        model_info = {\n",
    "            \"state_dict\": model_state_dict,\n",
    "            \"network_type\": \"\",\n",
    "            \"state_dim\": self.state_dim,\n",
    "        }\n",
    "        torch.save(model_info, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"加载模型和网络配置\"\"\"\n",
    "        model_info = torch.load(path, map_location=self.device)\n",
    "        # 加载模型参数\n",
    "        if self.distributed:\n",
    "            self.policy.module.load_state_dict(model_info[\"state_dict\"])\n",
    "        else:\n",
    "            self.policy.load_state_dict(model_info[\"state_dict\"])\n",
    "        self.policy.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               idx  date   repl_date  leadtime  actual  pred_y  \\\n",
      "0  59686521620-565     0  2025-01-16         2       0     1.0   \n",
      "1  59686521620-565     1  2025-01-17         2       1     1.0   \n",
      "2  59686521620-565     2  2025-01-18         2       3     1.0   \n",
      "3  59686521620-565     3  2025-01-19         2       1     1.0   \n",
      "4  59686521620-565     4  2025-01-20         2       1     1.0   \n",
      "5  59686521620-565     5  2025-01-21         2       2     1.0   \n",
      "6  59686521620-565     6  2025-01-22         2       0     1.0   \n",
      "7  59686521620-565     7  2025-01-23         2       1     2.0   \n",
      "8  59686521620-565     8  2025-01-24         2       0     1.0   \n",
      "9  59686521620-565     9  2025-01-25         2       0     1.0   \n",
      "\n",
      "                 predicted_demand  initial_stock     model_id last_soc  ...  \\\n",
      "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "4  [1.0, 1.0, 1.0, 1.0, 1.0, 2.0]            0.0  59686521620      565  ...   \n",
      "5  [1.0, 1.0, 1.0, 1.0, 2.0, 1.0]            0.0  59686521620      565  ...   \n",
      "6  [1.0, 1.0, 1.0, 2.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "7  [1.0, 1.0, 2.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "8  [1.0, 2.0, 1.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "9  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]            0.0  59686521620      565  ...   \n",
      "\n",
      "   std_item_qty_7d_lag1  std_item_qty_14d_lag1  \\\n",
      "0              0.899735               0.825420   \n",
      "1              0.899735               0.841897   \n",
      "2              0.755929               0.825420   \n",
      "3              1.154701               0.997249   \n",
      "4              1.069045               0.997249   \n",
      "5              1.000000               0.960769   \n",
      "6              1.069045               0.960769   \n",
      "7              1.069045               0.960769   \n",
      "8              0.951190               0.916875   \n",
      "9              1.069045               0.916875   \n",
      "\n",
      "   trend_item_qty_3d_minus_14d_lag1  trend_item_qty_3d_minus_7d_lag1  \\\n",
      "0                         -0.380952                        -0.523810   \n",
      "1                         -0.309524                        -0.523810   \n",
      "2                         -0.380952                        -0.380952   \n",
      "3                          0.404762                         0.333333   \n",
      "4                          0.738095                         0.809524   \n",
      "5                          0.666667                         0.666667   \n",
      "6                          0.333333                         0.190476   \n",
      "7                          0.000000                        -0.142857   \n",
      "8                         -0.071429                        -0.285714   \n",
      "9                         -0.595238                        -0.809524   \n",
      "\n",
      "  trend_item_qty_3d_minus_5d_lag1  forecast_error_lag1  forecast_label_lag1  \\\n",
      "0                             0.0                  1.0              02:over   \n",
      "1                             0.0                  0.0              01:same   \n",
      "2                             0.0                 -1.0             03:under   \n",
      "3                             0.0                 -3.0             03:under   \n",
      "4                             0.0                 -1.0             03:under   \n",
      "5                             0.0                 -1.0             03:under   \n",
      "6                             0.0                 -1.0             03:under   \n",
      "7                             0.0                  1.0              02:over   \n",
      "8                             0.0                  0.0              01:same   \n",
      "9                             0.0                  1.0              02:over   \n",
      "\n",
      "   avg_forecast_error_last_3d_lag1  avg_forecast_error_last_5d_lag1  \\\n",
      "0                         0.333333                            -0.25   \n",
      "1                         0.000000                            -0.20   \n",
      "2                         0.000000                             0.00   \n",
      "3                        -1.333333                            -0.80   \n",
      "4                        -1.666667                            -0.80   \n",
      "5                        -1.666667                            -1.20   \n",
      "6                        -1.000000                            -1.40   \n",
      "7                        -0.333333                            -1.00   \n",
      "8                         0.000000                            -0.40   \n",
      "9                         0.666667                             0.00   \n",
      "\n",
      "   avg_forecast_error_last_7d_lag1  \n",
      "0                        -0.250000  \n",
      "1                        -0.200000  \n",
      "2                        -0.333333  \n",
      "3                        -0.714286  \n",
      "4                        -0.571429  \n",
      "5                        -0.857143  \n",
      "6                        -0.857143  \n",
      "7                        -0.857143  \n",
      "8                        -0.857143  \n",
      "9                        -0.571429  \n",
      "\n",
      "[10 rows x 36 columns]\n",
      "Process 0/1: #############load dataset#############\n",
      "Process 0/1: #############data loaded#############\n",
      "Process 0/1: #############gen data#############\n",
      "Process 0/1: #############gen data done#############\n",
      "8\n",
      "[0.0, 0, 0, 0, 0, 0, 2, 1.0]\n",
      "[0.0, 0, 0, 0, 0, 0, 2, 3.0]\n",
      "(2,)\n",
      "[7 6]\n",
      "[[0.12013762 0.10723525 0.1080694  0.12380104 0.11115903 0.14174554\n",
      "  0.13807629 0.14977579]\n",
      " [0.1281812  0.09438679 0.09851526 0.12353233 0.11343302 0.13322\n",
      "  0.17156318 0.13716811]]\n",
      "Process 0/1: #############load dataset#############\n",
      "Process 0/1: #############data loaded#############\n",
      "Process 0/1: #############gen data#############\n",
      "Process 0/1: #############gen data done#############\n"
     ]
    }
   ],
   "source": [
    "# 率先定义onnx模型\n",
    "onnx_path = \"/home/work/apb-project/ais-deploy-demo-cache/replenishment-service-file/rl/id/default_test/model.onnx\"\n",
    "\n",
    "# 读数据\n",
    "cfg_path = \"/home/work/apb-project/ais-deploy-demo-cache/replenishment-service-file/rl/id/default_test/model.json\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "#cfg[\"data_path\"] = \"/home/work/apb-project/ais-deploy-demo-cache/replenishment/data/20250319/dwd_cache_replenish_rl_test_dataset_1k_idx_forward_period_g1\"\n",
    "a = pd.read_parquet(cfg[\"data_path\"])\n",
    "print(a.head(10))\n",
    "ra = ReplenishAgent_make_date(None, cfg)\n",
    "#ra.load_model(model_path)\n",
    "state_map = {sku: ra.reset_network_state(sku) for sku in ra.sku_id_ls}\n",
    "\n",
    "\n",
    "\n",
    "# onnx模型的预测流程，暂时注释，后续再启动\n",
    "# 加载onnx模型\n",
    "sess_options = ort.SessionOptions()\n",
    "# 设置单个操作内部使用的线程数\n",
    "sess_options.intra_op_num_threads = 10\n",
    "# 设置是否顺序执行操作图内部的算子，还是并行执行\n",
    "sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "onnx_model = ort.InferenceSession(onnx_path, sess_options=sess_options)\n",
    "onnx_sku_list = []\n",
    "onnx_action_list = []\n",
    "onnx_action_proba_list = []\n",
    "for sku in ra.sku_id_ls[0:2]:\n",
    "    print(state_map[sku])\n",
    "    onnx_action = onnx_model.run(None,{'x':np.array(state_map[sku])[np.newaxis,:].astype(np.float32)})[0]\n",
    "    onnx_action_list.append(np.argmax(onnx_action))\n",
    "    onnx_action_proba_list.append(onnx_action)\n",
    "    onnx_sku_list.append(sku)\n",
    "\n",
    "onnx_sku = np.array(onnx_sku_list)\n",
    "onnx_actions = np.array(onnx_action_list)\n",
    "onnx_actions_proba = np.concatenate(onnx_action_proba_list, axis=0)\n",
    "print(onnx_sku.shape)\n",
    "print(onnx_actions)\n",
    "print(onnx_actions_proba)\n",
    "# 加载数据\n",
    "datasets = Datasets(\"/home/work/apb-project/ais-deploy-demo-cache/replenishment/data/20250319/dwd_cache_replenish_rl_test_dataset_1k_idx_forward_period_g1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end_of_stock': 0.0, 'transit_stock_0': 0.0, 'transit_stock_1': 0.0, 'transit_stock_2': 0.0, 'transit_stock_3': 0.0, 'transit_stock_4': 0.0, 'leadtime': 2.0, 'pred_y': 1.0}\n",
      "{'end_of_stock': 0.0, 'transit_stock_0': 0.0, 'transit_stock_1': 0.0, 'transit_stock_2': 0.0, 'transit_stock_3': 0.0, 'transit_stock_4': 0.0, 'leadtime': 2.0, 'pred_y': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "ModelVersion = \"default_test\"\n",
    "columns = cfg[\"columns\"]\n",
    "tmp_list = []\n",
    "tmp_dict = {}\n",
    "tmp_dict[\"logid\"] = 123456\n",
    "tmp_dict[\"clientip\"] = \"winning\"\n",
    "tmp_dict[\"data\"] = {\"ModelVersion\": ModelVersion}\n",
    "\n",
    "for sku in ra.sku_id_ls[0:2]:\n",
    "    feature = dict(zip(columns, [float(i) for i in state_map[sku]]))\n",
    "    print(feature)\n",
    "    temp_dict = {}\n",
    "    temp_dict[\"info\"] = {\"unique_id\": sku, \"outlier_detection\":0, \"split_2_daily\":0}\n",
    "    temp_dict[\"features\"] = feature\n",
    "    tmp_list.append(temp_dict)\n",
    "tmp_dict[\"data\"][\"input\"] = tmp_list\n",
    "\n",
    "url = \"http://127.0.0.1:1145/api/process\"\n",
    "#url = \"http://sg10.aip.mlp.shopee.io/aip-svc-100113/rl-20250324/api/process\"\n",
    "req = json.dumps(tmp_dict)\n",
    "with open(\"/home/work/apb-project/rl_req.json\",'w') as f:\n",
    "    f.write(req)\n",
    "    f.close()\n",
    "res=requests.post(url = url, data=req)\n",
    "\n",
    "b = json.dumps(res.json())\n",
    "with open(\"/home/work/apb-project/rl_req.json\",'w') as f:\n",
    "    f.write(req)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"err_num\": 0, \"err_msg\": \"OK\", \"host\": \"vs-0224df45-190f-4e64-b2fe-80ea263d8380-4182861100-f448f97bjt8q\", \"result\": {\"output\": [{\"unique_id\": \"10046901900-18299\", \"output\": {\"multiplier\": 3, \"probability\": [0.12013761699199677, 0.10723525285720825, 0.10806940495967865, 0.12380103766918182, 0.11115903407335281, 0.14174553751945496, 0.1380762904882431, 0.14977578818798065]}}, {\"unique_id\": \"100578503849-14461\", \"output\": {\"multiplier\": 2, \"probability\": [0.12818120419979095, 0.09438678622245789, 0.09851526468992233, 0.12353232502937317, 0.11343301832675934, 0.13322000205516815, 0.17156317830085754, 0.13716810941696167]}}], \"ModelVersion\": \"default_test\"}}'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/work/apb-project/ais-deploy-demo-cache/replenishment/server/rl_online_test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ais.mlp.shopee.io/home/work/apb-project/ais-deploy-demo-cache/replenishment/server/rl_online_test.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m onnx_action_proba_list \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m onnx_action_proba_list]\n",
      "\u001b[1;32m/home/work/apb-project/ais-deploy-demo-cache/replenishment/server/rl_online_test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ais.mlp.shopee.io/home/work/apb-project/ais-deploy-demo-cache/replenishment/server/rl_online_test.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m onnx_action_proba_list \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39;49msqueeze()\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m onnx_action_proba_list]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "onnx_action_proba_list = [i.squeeze().tolist() for i in onnx_action_proba_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = res.json()['result']['output']\n",
    "predict_list = [pd.DataFrame(i) for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11580/3736755721.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  a[\"offline_output\"] = onnx_action_proba_list\n"
     ]
    }
   ],
   "source": [
    "predict_df = pd.concat(predict_list).reset_index(names=\"value\")\n",
    "a = predict_df.query(\"value=='probability'\")\n",
    "a[\"offline_output\"] = onnx_action_proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>output</th>\n",
       "      <th>offline_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probability</td>\n",
       "      <td>101724530018-26</td>\n",
       "      <td>[0.166314959526062, 0.1331610530614853, 0.1388...</td>\n",
       "      <td>[0.166314959526062, 0.1331610530614853, 0.1388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probability</td>\n",
       "      <td>102460392589-26</td>\n",
       "      <td>[0.19042392075061798, 0.12624157965183258, 0.1...</td>\n",
       "      <td>[0.19042392075061798, 0.12624157965183258, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probability</td>\n",
       "      <td>104185028940-14461</td>\n",
       "      <td>[0.16609469056129456, 0.13671071827411652, 0.1...</td>\n",
       "      <td>[0.16609469056129456, 0.13671071827411652, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>probability</td>\n",
       "      <td>104357715697-9820</td>\n",
       "      <td>[0.1883750855922699, 0.12450452893972397, 0.13...</td>\n",
       "      <td>[0.1883750855922699, 0.12450452893972397, 0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>probability</td>\n",
       "      <td>105514554826-2240</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>probability</td>\n",
       "      <td>98986994632-14461</td>\n",
       "      <td>[0.166314959526062, 0.1331610530614853, 0.1388...</td>\n",
       "      <td>[0.166314959526062, 0.1331610530614853, 0.1388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>probability</td>\n",
       "      <td>98987885103-10631</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>probability</td>\n",
       "      <td>99588195830-1581</td>\n",
       "      <td>[0.20344261825084686, 0.12024938315153122, 0.1...</td>\n",
       "      <td>[0.20344261825084686, 0.12024938315153122, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>probability</td>\n",
       "      <td>99600246322-2240</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>probability</td>\n",
       "      <td>99670062948-764</td>\n",
       "      <td>[0.19042392075061798, 0.12624157965183258, 0.1...</td>\n",
       "      <td>[0.19042392075061798, 0.12624157965183258, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            value           unique_id  \\\n",
       "1     probability     101724530018-26   \n",
       "3     probability     102460392589-26   \n",
       "5     probability  104185028940-14461   \n",
       "7     probability   104357715697-9820   \n",
       "9     probability   105514554826-2240   \n",
       "...           ...                 ...   \n",
       "1991  probability   98986994632-14461   \n",
       "1993  probability   98987885103-10631   \n",
       "1995  probability    99588195830-1581   \n",
       "1997  probability    99600246322-2240   \n",
       "1999  probability     99670062948-764   \n",
       "\n",
       "                                                 output  \\\n",
       "1     [0.166314959526062, 0.1331610530614853, 0.1388...   \n",
       "3     [0.19042392075061798, 0.12624157965183258, 0.1...   \n",
       "5     [0.16609469056129456, 0.13671071827411652, 0.1...   \n",
       "7     [0.1883750855922699, 0.12450452893972397, 0.13...   \n",
       "9     [0.1876506209373474, 0.12993977963924408, 0.15...   \n",
       "...                                                 ...   \n",
       "1991  [0.166314959526062, 0.1331610530614853, 0.1388...   \n",
       "1993  [0.1876506209373474, 0.12993977963924408, 0.15...   \n",
       "1995  [0.20344261825084686, 0.12024938315153122, 0.1...   \n",
       "1997  [0.1876506209373474, 0.12993977963924408, 0.15...   \n",
       "1999  [0.19042392075061798, 0.12624157965183258, 0.1...   \n",
       "\n",
       "                                         offline_output  \n",
       "1     [0.166314959526062, 0.1331610530614853, 0.1388...  \n",
       "3     [0.19042392075061798, 0.12624157965183258, 0.1...  \n",
       "5     [0.16609469056129456, 0.13671071827411652, 0.1...  \n",
       "7     [0.1883750855922699, 0.12450452893972397, 0.13...  \n",
       "9     [0.1876506209373474, 0.12993977963924408, 0.15...  \n",
       "...                                                 ...  \n",
       "1991  [0.166314959526062, 0.1331610530614853, 0.1388...  \n",
       "1993  [0.1876506209373474, 0.12993977963924408, 0.15...  \n",
       "1995  [0.20344261825084686, 0.12024938315153122, 0.1...  \n",
       "1997  [0.1876506209373474, 0.12993977963924408, 0.15...  \n",
       "1999  [0.19042392075061798, 0.12624157965183258, 0.1...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multiplier</td>\n",
       "      <td>101724530018-26</td>\n",
       "      <td>[1.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probability</td>\n",
       "      <td>101724530018-26</td>\n",
       "      <td>[0.166314959526062, 0.1331610530614853, 0.1388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multiplier</td>\n",
       "      <td>102460392589-26</td>\n",
       "      <td>[1.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probability</td>\n",
       "      <td>102460392589-26</td>\n",
       "      <td>[0.19042392075061798, 0.12624157965183258, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multiplier</td>\n",
       "      <td>104185028940-14461</td>\n",
       "      <td>[1.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>probability</td>\n",
       "      <td>99588195830-1581</td>\n",
       "      <td>[0.20344261825084686, 0.12024938315153122, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>multiplier</td>\n",
       "      <td>99600246322-2240</td>\n",
       "      <td>[1.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>probability</td>\n",
       "      <td>99600246322-2240</td>\n",
       "      <td>[0.1876506209373474, 0.12993977963924408, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>multiplier</td>\n",
       "      <td>99670062948-764</td>\n",
       "      <td>[1.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>probability</td>\n",
       "      <td>99670062948-764</td>\n",
       "      <td>[0.19042392075061798, 0.12624157965183258, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            value           unique_id  \\\n",
       "0      multiplier     101724530018-26   \n",
       "1     probability     101724530018-26   \n",
       "2      multiplier     102460392589-26   \n",
       "3     probability     102460392589-26   \n",
       "4      multiplier  104185028940-14461   \n",
       "...           ...                 ...   \n",
       "1995  probability    99588195830-1581   \n",
       "1996   multiplier    99600246322-2240   \n",
       "1997  probability    99600246322-2240   \n",
       "1998   multiplier     99670062948-764   \n",
       "1999  probability     99670062948-764   \n",
       "\n",
       "                                                 output  \n",
       "0                                                 [1.8]  \n",
       "1     [0.166314959526062, 0.1331610530614853, 0.1388...  \n",
       "2                                                 [1.8]  \n",
       "3     [0.19042392075061798, 0.12624157965183258, 0.1...  \n",
       "4                                                 [1.8]  \n",
       "...                                                 ...  \n",
       "1995  [0.20344261825084686, 0.12024938315153122, 0.1...  \n",
       "1996                                              [1.8]  \n",
       "1997  [0.1876506209373474, 0.12993977963924408, 0.15...  \n",
       "1998                                              [1.8]  \n",
       "1999  [0.19042392075061798, 0.12624157965183258, 0.1...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"output\"] == a[\"offline_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}\n",
      "{'info': {'unique_id': '102460392589-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 2, 0, 0, 0, 0, 0]}}\n",
      "{'ModelVersion': '20250324', 'input': [{'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '102460392589-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 2, 0, 0, 0, 0, 0]}}]}\n"
     ]
    }
   ],
   "source": [
    "json_path = \"/home/work/apb-project/rl_req.json\"\n",
    "with open(json_path,'rb') as f:\n",
    "    inputs = json.load(f)\n",
    "print(inputs[\"data\"][\"input\"][0])\n",
    "print(inputs[\"data\"][\"input\"][1])\n",
    "print(inputs[\"data\"])\n",
    "raw = inputs[\"data\"][\"input\"][0]\n",
    "\n",
    "inputs[\"data\"][\"input\"] = [raw for i in range(16)]\n",
    "req = json.dumps(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelVersion': '20250324', 'input': [{'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}, {'info': {'unique_id': '101724530018-26', 'outlier_detection': 0, 'split_2_daily': 0}, 'features': {'input': [0.0, 0, 1.0, 1, 0, 0, 0, 0, 0]}}]}\n",
      "{'err_num': 0, 'err_msg': 'OK', 'host': 'rl-20250324-39934900002-86c8b55579-8gjbp', 'result': {'output': [{'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}, {'unique_id': '101724530018-26', 'output': {'multiplier': 1.8, 'probability': [0.166314959526062, 0.1331610530614853, 0.13880473375320435, 0.13573363423347473, 0.17303362488746643, 0.11430231481790543, 0.10010004043579102, 0.03854963928461075]}}], 'ModelVersion': '20250324'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://sg10.aip.mlp.shopee.io/aip-svc-100113/rl-20250324/api/process\"\n",
    "res=requests.post(url = url, data=req)\n",
    "#print(inputs[\"data\"])\n",
    "print(res.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
